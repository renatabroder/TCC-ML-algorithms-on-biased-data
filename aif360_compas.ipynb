{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aif360 - compas.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Gys4VcqkiJvi",
        "Bw9kSK80iRw0",
        "mi9HWHLmi3FP",
        "5FnQeC0zkKRj",
        "4EG4X_Jbvk_L",
        "PeMhXJ3LzuNI",
        "Uu8TSPJ8OGK8",
        "xh-GJZxqRR0J"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suiSjCDDW-WG"
      },
      "source": [
        "# Análise de Desempenho em Algoritmos de Aprendizado de Máquina Treinados em Dados com Viés\n",
        "\n",
        "Renata Broder\n",
        "\n",
        "Lilian Berton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqPrp5oYhst"
      },
      "source": [
        "## Configurações iniciais\n",
        "\n",
        "Esta seção dedica-se à importação das bibliotecas necessárias para os experimentos que serão conduzidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gys4VcqkiJvi"
      },
      "source": [
        "### Pip install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTlIELOzcqKz",
        "outputId": "5e0ab5de-d116-4757-8c40-08c4c936a6ff"
      },
      "source": [
        "pip install aif360"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from aif360) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from aif360) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from aif360) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from aif360) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.6/dist-packages (from aif360) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21->aif360) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->aif360) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59r-1T17d85u",
        "outputId": "03e0c5bd-41a4-4d63-b65a-06465daa0288"
      },
      "source": [
        "pip install 'aif360[AdversarialDebiasing]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aif360[AdversarialDebiasing] in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (1.1.5)\n",
            "Requirement already satisfied: tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\" in /usr/local/lib/python3.6/dist-packages (from aif360[AdversarialDebiasing]) (1.15.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21->aif360[AdversarialDebiasing]) (1.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[AdversarialDebiasing]) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[AdversarialDebiasing]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[AdversarialDebiasing]) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[AdversarialDebiasing]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360[AdversarialDebiasing]) (2018.9)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.32.0)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (53.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.13.1; extra == \"adversarialdebiasing\"->aif360[AdversarialDebiasing]) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FDeZXCIR5nc",
        "outputId": "e1b73cfc-c200-4951-b4b8-f5dd722e8c2f"
      },
      "source": [
        "pip install 'aif360[LFR]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aif360[LFR] in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.42.0; extra == \"lfr\" in /usr/local/lib/python3.6/dist-packages (from aif360[LFR]) (0.51.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21->aif360[LFR]) (1.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[LFR]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[LFR]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360[LFR]) (2.4.7)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.42.0; extra == \"lfr\"->aif360[LFR]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.42.0; extra == \"lfr\"->aif360[LFR]) (53.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360[LFR]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEtjF8iUVNRV",
        "outputId": "afc6e64a-8ffd-4bab-c7ef-7ad134c8266a"
      },
      "source": [
        "pip install BlackBoxAuditing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: BlackBoxAuditing in /usr/local/lib/python3.6/dist-packages (0.1.54)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing) (2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from BlackBoxAuditing) (3.2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->BlackBoxAuditing) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->BlackBoxAuditing) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->BlackBoxAuditing) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw9kSK80iRw0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVRlAsjDg-U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba1d80f-9e41-456b-b254-0f73824a4453"
      },
      "source": [
        "%matplotlib inline\n",
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset, StandardDataset\n",
        "from aif360.datasets import CompasDataset\n",
        "\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "\n",
        "# from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "            import load_preproc_data_compas\n",
        "# from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
        "#             import get_distortion_adult, get_distortion_german, get_distortion_compas\n",
        "# from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:No module named 'numba.decorators': LFR will be unavailable. To install, run:\n",
            "pip install 'aif360[LFR]'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNIh1uuzt5Cj",
        "outputId": "22f48f30-2307-459a-fa1e-cd4d639d602b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LDXfOBDY9g-"
      },
      "source": [
        "## Load do *COMPAS Dataset* e pré processamento inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dhI5qDGH7WA"
      },
      "source": [
        "Importar os arquivos do adult dataset do UCI para a pasta para a biblioteca aif360 poder realizar a leitura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCJ9uZg_HJdQ",
        "outputId": "d4d9e232-ca76-405e-a7ed-72cde213341a"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('/usr/local/lib/python3.6/dist-packages/aif360/data/raw/compas/compas-scores-two-years.csv', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2546489"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ti71feqQpWA"
      },
      "source": [
        "Por dentro da função load_preproc_data_adult:\n",
        "\n",
        "```python\n",
        "BLABLA\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "O custom_preprocessing é o pré processamento de dados que é realizado especialmente para o **Adult Dataset**:\n",
        "\n",
        "```python\n",
        "BLABLABLA2\n",
        "\n",
        "```\n",
        "\n",
        "  Além deste pré processamento customizado que consiste no agrupamento dos atributos 'Education Years', 'Age', além do tratamento das categorias dos atributos já mencionados, bem como dos atributos 'Sex' e 'Race' (passo 1), também é realizado um pré processamento na classe **StandardDataset** a partir dos passos seguintes:\n",
        "2. Remoção de colunas não especificadas;\n",
        "3. Remoção de linhas que contém Na\n",
        "4. Tratamento para os dados categóricos\n",
        "5. Binarização dos labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_lbb3bOZ8ju"
      },
      "source": [
        "## Classe desfavorecida: **Gênero e Raça**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUqF-AMZQgYM"
      },
      "source": [
        "# Setar nome variável que seá utilizada no nome dos arquivos gerados\n",
        "name_unprivileged_groups = 'Raça' #Gênero e Raça\n",
        "\n",
        "# Configurar gênero como classe desprivilegiada\n",
        "privileged_groups = [{'race': 1}]#, {'sex': 1, 'race': 1}]\n",
        "unprivileged_groups = [{'race': 0}]#, {'sex': 0, 'race': 0}]\n",
        "\n",
        "list_unprivileged_groups = ['race'] #'sex', 'race']\n",
        "\n",
        "# Carregar o dataset já pré processado conforme processo descrito na seção anterior \n",
        "df_compas = load_preproc_data_compas(list_unprivileged_groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPmoxijYktw4",
        "outputId": "59a12cf0-23c3-4507-8a5c-285af6d7232c"
      },
      "source": [
        "print(df_compas.feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sex', 'race', 'age_cat=25 to 45', 'age_cat=Greater than 45', 'age_cat=Less than 25', 'priors_count=0', 'priors_count=1 to 3', 'priors_count=More than 3', 'c_charge_degree=F', 'c_charge_degree=M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9CHZMAqBOBe"
      },
      "source": [
        "Agora a variável `df_compas` contém o dataset já pré processado. Vamos reutilizar este *dataset* para compor diferentes grupos de treino e teste gerados com diferentes *seeds* e medir a média da paridade demográfica "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGOIPgtABqI6",
        "outputId": "0a693e6d-01d0-47a4-bb39-edee55c3ab41"
      },
      "source": [
        "#random seed\n",
        "np.random.seed(13)\n",
        "\n",
        "# seeds = np.round((np.random.rand(30) * 1000))\n",
        "\n",
        "seeds = [ 36, 184, 892, 707, 826, 715, 777, 251, 740, 134, 676, 708, 839, 343,\n",
        " 393,  39, 937, 478, 325, 532, 161, 462, 660, 378, 872, 355, 636, 673,\n",
        " 447,  34]\n",
        "print(seeds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 184, 892, 707, 826, 715, 777, 251, 740, 134, 676, 708, 839, 343, 393, 39, 937, 478, 325, 532, 161, 462, 660, 378, 872, 355, 636, 673, 447, 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtNHmR2NiV7s"
      },
      "source": [
        "### Funções auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi9HWHLmi3FP"
      },
      "source": [
        "#### Métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJswktklhVmf"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "def compute_metrics(dataset_true, dataset_pred, \n",
        "                    unprivileged_groups, privileged_groups,\n",
        "                    best_class = -1,\n",
        "                    disp = False):\n",
        "    \"\"\" Compute the key metrics \"\"\"\n",
        "    m = ClassificationMetric(dataset_true,\n",
        "                             dataset_pred, \n",
        "                             unprivileged_groups=unprivileged_groups,\n",
        "                             privileged_groups=privileged_groups)\n",
        "    metrics = OrderedDict()\n",
        "    metrics[\"Accuracy\"] = m.accuracy()\n",
        "    metrics[\"Balanced accuracy\"] = 0.5*(m.true_positive_rate() + m.true_negative_rate())\n",
        "    metrics[\"Precision\"] = m.precision()\n",
        "    metrics[\"Recall\"] = m.recall()\n",
        "    metrics[\"F1-Score\"] = (2 * metrics[\"Precision\"] * metrics[\"Recall\"]) / (metrics[\"Precision\"] + metrics[\"Recall\"])\n",
        "    metrics[\"Statistical parity difference\"] = m.statistical_parity_difference()\n",
        "    metrics[\"Disparate impact\"] = m.disparate_impact()\n",
        "    metrics[\"Average odds difference\"] = m.average_odds_difference()\n",
        "    metrics[\"Equal opportunity difference\"] = m.equal_opportunity_difference()    \n",
        "    metrics[\"Class Threshold\"] = best_class\n",
        "    \n",
        "    if disp:\n",
        "        for k in metrics:\n",
        "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
        "    \n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpP14ke5gsHy"
      },
      "source": [
        "def avg_metrics_seeds(metricas, name):\n",
        "    # calculo das métricas para todos os testes\n",
        "    m_bal_acc = []\n",
        "    m_stat_par_diff = []\n",
        "    m_disp_imp = []\n",
        "    m_avg_odd_diff = []\n",
        "    m_eq_opp_diff = []\n",
        "    m_acc = []\n",
        "    m_prec = []\n",
        "    m_rec = []\n",
        "    m_f1 = []\n",
        "\n",
        "\n",
        "    df_metricas = pd.DataFrame.from_dict(metricas, orient='columns')\n",
        "\n",
        "    for i, s in enumerate(metricas):\n",
        "        m_acc.append(s['Accuracy'])\n",
        "        m_avg_odd_diff.append(s['Average odds difference'])\n",
        "        m_bal_acc.append(s['Balanced accuracy'])\n",
        "        m_disp_imp.append(s['Disparate impact'])\n",
        "        m_eq_opp_diff.append(s['Equal opportunity difference'])\n",
        "        m_f1.append(s['F1-Score'])\n",
        "        m_prec.append(s['Precision'])\n",
        "        m_rec.append(s['Recall'])\n",
        "        m_stat_par_diff.append(s['Statistical parity difference'])\n",
        "\n",
        "    df_mean = pd.DataFrame({\n",
        "        'Accuracy': round(np.mean(m_acc), 3),\n",
        "        'Balanced accuracy': round(np.mean(m_bal_acc), 3),\n",
        "        'Average odds difference': round(np.mean(m_avg_odd_diff), 3),\n",
        "        'Balanced accuracy': round(np.mean(m_bal_acc), 3),\n",
        "        'Disparate impact': round(np.mean(m_disp_imp), 3),\n",
        "        'Equal opportunity difference': round(np.mean(m_eq_opp_diff), 3),\n",
        "        'F1-Score': round(np.mean(m_f1), 3),\n",
        "        'Precision': round(np.mean(m_prec), 3),\n",
        "        'Recall': round(np.mean(m_rec), 3),\n",
        "        'Statistical parity difference': round(np.mean(m_stat_par_diff), 3),\n",
        "    }, index=['Media'])\n",
        "\n",
        "    #df_aux = [np.mean(m_bal_acc), np.mean(m_avg_odd_diff), np.mean(m_bal_acc)]\n",
        "\n",
        "    df_std = pd.DataFrame({\n",
        "        'Accuracy': round(np.std(m_acc), 3),\n",
        "        'Balanced accuracy': round(np.std(m_bal_acc), 3),\n",
        "        'Average odds difference': round(np.std(m_avg_odd_diff), 3),\n",
        "        'Balanced accuracy': round(np.std(m_bal_acc), 3),\n",
        "        'Disparate impact': round(np.std(m_disp_imp), 3),\n",
        "        'Equal opportunity difference': round(np.std(m_eq_opp_diff), 3),\n",
        "        'F1-Score': round(np.std(m_f1), 3),\n",
        "        'Precision': round(np.std(m_prec), 3),\n",
        "        'Recall': round(np.std(m_rec), 3),\n",
        "        'Statistical parity difference': round(np.std(m_stat_par_diff), 3),\n",
        "    }, index=['DesvPad'])\n",
        "\n",
        "    df_metricas = df_metricas.append(df_mean)\n",
        "    df_metricas = df_metricas.append(df_std)\n",
        "\n",
        "    df_metricas.to_csv(f'/content/drive/My Drive/UNIFESP/TCC/Arquivos/{name}.csv')\n",
        "\n",
        "    metricas = {\n",
        "        'ACC': np.mean(m_acc),\n",
        "        'AOD': np.mean(m_avg_odd_diff),\n",
        "        'BA': np.mean(m_bal_acc),\n",
        "        'DI': np.mean(m_disp_imp),\n",
        "        'EOD': np.mean(m_eq_opp_diff),\n",
        "        'F1': np.mean(m_f1),\n",
        "        'PRC': np.mean(m_prec),\n",
        "        'REC': np.mean(m_rec),\n",
        "        'SPD': np.mean(m_stat_par_diff),\n",
        "    }\n",
        "\n",
        "    display(Markdown(\"#### Acurácia (Objetivo: 1)\"))\n",
        "    print(metricas['ACC'])\n",
        "\n",
        "    display(Markdown(\"#### Acurácia Balanceada (Objetivo: 1)\"))\n",
        "    print(metricas['BA'])\n",
        "\n",
        "    display(Markdown(\"#### Diferença Paridade estatística (Objetivo: 0)\"))\n",
        "    print(metricas['SPD'])\n",
        "\n",
        "    display(Markdown(\"#### Impacto diferenciado (Objetivo: 1)\"))\n",
        "    print(metricas['DI'])\n",
        "\n",
        "    display(Markdown(\"#### F1 Score (Objetivo: 1)\"))\n",
        "    print(metricas['F1'])\n",
        "\n",
        "    display(Markdown(\"#### Recall (Objetivo: 1)\"))\n",
        "    print(metricas['REC'])\n",
        "\n",
        "    display(Markdown(\"#### Diferença de probabilidades (Objetivo: 0)\"))\n",
        "    print(metricas['AOD'])\n",
        "\n",
        "    display(Markdown(\"#### Equal opportunities difference (Objetivo: 0)\"))\n",
        "    print(metricas['EOD'])\n",
        "\n",
        "    return metricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks7RmGrFO9Uq"
      },
      "source": [
        "def comparar_metricas(metricas, titulo, name):\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "\n",
        "    nome_metricas = ['Accuracy', 'Average Odds Difference', 'Balanced Accuracy', 'Disparate Impact', 'Equal Opportunities Difference', 'F1-Score', 'Precision', 'Recall', 'Statistical Parity Difference']\n",
        "    objetivo =      [         1,                         0,                   1,                  1,                                0,          1,           1,        1,                               0]\n",
        "    limites =       [  [0, 1.5],                   [-1, 1],            [0, 1.5],           [0, 1.5],                          [-1, 1],   [0, 1.5],    [0, 1.5], [0, 1.5],                         [-1, 1]]\n",
        "\n",
        "    for i, m in enumerate(metricas[0][0]):\n",
        "        #(i, m) = 0 BA, 1 SPD ...\n",
        "        x = np.arange(len(metricas))  # the label locations\n",
        "        width = 0.35  # the width of the bars\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 5))\n",
        "        fig.patch.set_facecolor((1,1,1)) # color background\n",
        "\n",
        "        ax.set_title(name) # Título do gráfico\n",
        "        ax.set_xticks([]) # Definir que não tem ticks no eixo x\n",
        "        ax.axhline(objetivo[i], label='Goal', color='k', linestyle=':')\n",
        "\n",
        "        ax.set_xlabel(f'{nome_metricas[i]}')\n",
        "        plt.ylim(limites[i][0], limites[i][1])\n",
        "\n",
        "        for i, conjunto in enumerate(metricas):\n",
        "            aux = ax.bar(x[i], round(conjunto[0][m], 3), width, label=conjunto[1])\n",
        "            autolabel(aux)\n",
        "\n",
        "        ax.legend()\n",
        "        fig.tight_layout()\n",
        "        # plt.show()\n",
        "        fig.savefig(f'/content/drive/My Drive/UNIFESP/TCC/Gráficos/{name} - {titulo}_{m}.png') \n",
        "        plt.clf()\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKn1ybTi6P0"
      },
      "source": [
        "#### Treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr6Upj0YJ2IT"
      },
      "source": [
        "def treino_valid_teste(df, clf, scaler, name, valid=True):\n",
        "    melhor_acur_bal = np.zeros(30)\n",
        "    melhor_lim_class = np.zeros(30)\n",
        "\n",
        "    metricas = []\n",
        "    metricas_seeds_sem_valid = []\n",
        "    metricas_seeds = []\n",
        "\n",
        "    for i, s in enumerate(seeds):\n",
        "        np.random.seed(s)\n",
        "\n",
        "        # Separação entre conjunto de teste, treino e validação\n",
        "        df_train, df_vt = df.split([0.7], shuffle=True)\n",
        "        df_valid, df_test = df_vt.split([0.5], shuffle=True)\n",
        "\n",
        "        x_train = scaler.fit_transform(df_train.features) # escalarização do dataset original para treino\n",
        "        y_train = df_train.labels.ravel() # label para o dataset de treino\n",
        "        w_train = df_train.instance_weights.ravel() # pesos iguais = 1\n",
        "\n",
        "\n",
        "\n",
        "        # Treino\n",
        "        clf.fit(x_train,\n",
        "                y_train,\n",
        "                sample_weight=w_train) # treino com todos os pesos iguais a 1\n",
        "\n",
        "        y_train_pred = clf.predict(x_train) # predição das mesmas entradas utilizadas para treino\n",
        "\n",
        "        df_train_pred = df_train.copy()  \n",
        "        df_train_pred.labels = y_train_pred # com os labels da predição feita com os dados de treino\n",
        "\n",
        "\n",
        "        pos_ind = np.where(clf.classes_ == df_train.favorable_label)[0][0] # pega índice da classe favoravel\n",
        "\n",
        "        # Validação\n",
        "        df_valid_pred = df_valid.copy(deepcopy=True) # cria cópia do conjunto de validação\n",
        "        x_valid = scaler.transform(df_valid_pred.features) # escalarização do conjunto\n",
        "        y_valid = df_valid_pred.labels # armazena labels na variável y_valid\n",
        "        df_valid_pred.scores = clf.predict_proba(x_valid)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "\n",
        "\n",
        "        # Classification threshold \"converte\" a probabilidade em classes binárias\n",
        "\n",
        "        # calcular acurácia balanceada e outras métricas para todos os limiares\n",
        "        num_thresh = 100\n",
        "        ba_arr = np.zeros(num_thresh)\n",
        "        class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
        "\n",
        "        for idx, class_thresh in enumerate(class_thresh_arr):\n",
        "            \n",
        "            fav_inds = df_valid_pred.scores > class_thresh # Se a probabilidade de classificar a instancia de validação como favorita for maior do que o limiar de classe,\n",
        "            df_valid_pred.labels[fav_inds] = df_valid_pred.favorable_label # classifica como favorável\n",
        "            df_valid_pred.labels[~fav_inds] = df_valid_pred.unfavorable_label # se não, classifica como desfavorável\n",
        "            \n",
        "            metricas.append(ClassificationMetric(df_valid,       # dataset de validação original\n",
        "                                                df_valid_pred,  # labels gerados pela probabilidade do classificador\n",
        "                                                unprivileged_groups=unprivileged_groups,\n",
        "                                                privileged_groups=privileged_groups))\n",
        "            \n",
        "            ba_arr[idx] = 0.5*(metricas[-1].true_positive_rate()\\\n",
        "                            +metricas[-1].true_negative_rate()) # calcula a média entre as taxas de verdadeiros positivos e negativos armazenando para cada limiar de classe\n",
        "\n",
        "        best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
        "        best_class_thresh = class_thresh_arr[best_ind]\n",
        "\n",
        "        melhor_acur_bal[i] = np.max(ba_arr)\n",
        "        melhor_lim_class[i] = np.max(best_class_thresh)\n",
        "\n",
        "\n",
        "        # Teste\n",
        "\n",
        "        df_test_pred = df_test.copy(deepcopy=True) # cria cópia do conjunto de teste\n",
        "        x_test = scaler.transform(df_test_pred.features) # escalarização do conjunto\n",
        "        y_test = df_test_pred.labels # armazena labels na variável y_test\n",
        "        df_test_pred.scores = clf.predict_proba(x_test)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "            \n",
        "        fav_inds = df_test_pred.scores > melhor_lim_class[i]\n",
        "        df_test_pred.labels[fav_inds] = df_test_pred.favorable_label\n",
        "        df_test_pred.labels[~fav_inds] = df_test_pred.unfavorable_label\n",
        "        \n",
        "        metric_test_bef = compute_metrics(df_test, df_test_pred, \n",
        "                                            unprivileged_groups, privileged_groups,\n",
        "                                            best_class_thresh)\n",
        "        metricas_seeds.append(metric_test_bef)\n",
        "\n",
        "\n",
        "        # Teste sem valid\n",
        "        df_test_pred.labels = clf.predict(x_test)\n",
        "        metric_test_sem_valid = compute_metrics(df_test, df_test_pred, \n",
        "                                                unprivileged_groups, privileged_groups)\n",
        "        metricas_seeds_sem_valid.append(metric_test_sem_valid)\n",
        "\n",
        "    metrica_df_gender = avg_metrics_seeds(metricas_seeds, name)\n",
        "    metrica_df_gender_sem_valid = avg_metrics_seeds(metricas_seeds_sem_valid, name + 'Sem validação')\n",
        "\n",
        "    return metrica_df_gender, metrica_df_gender_sem_valid, melhor_lim_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9LzFMsNbKL"
      },
      "source": [
        "def treino_RW(df, clf, scaler, bct, name):\n",
        "    RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "    metricas_seeds_rw = []\n",
        "\n",
        "    for i, s in enumerate(seeds):\n",
        "        np.random.seed(s)\n",
        "\n",
        "        # Separação entre conjunto de teste, treino e validação\n",
        "        df_train, df_test = df.split([0.7], shuffle=True)\n",
        "        # df_valid, df_test = df_vt.split([0.5], shuffle=True)\n",
        "\n",
        "        # Rebalanceamento dos pesos\n",
        "        df_train_rw = RW.fit_transform(df_train) # aplicado o reweight no conjunto de treino\n",
        "\n",
        "        index = [df_train.feature_names.index(u) for u in list_unprivileged_groups]\n",
        "\n",
        "        df_train_rw.features = scaler.fit_transform(df_train_rw.features)\n",
        "        x_train_rw = df_train_rw.features#np.delete(df_train_rw.features, index, axis=1)\n",
        "        y_train_rw = df_train_rw.labels.ravel()\n",
        "        w_train_rw = df_train_rw.instance_weights\n",
        "\n",
        "        # x_valid_rw = scaler.fit_transform(df_valid.features)\n",
        "        #x_valid_rw = np.delete(x_valid_rw, index, axis=1)\n",
        "\n",
        "        x_test_rw = scaler.fit_transform(df_test.features)\n",
        "        #x_test_rw = np.delete(x_test_rw, index, axis=1)\n",
        "\n",
        "\n",
        "        clf.fit(x_train_rw,\n",
        "                y_train_rw, \n",
        "                sample_weight=w_train_rw) # treino com Regressão Logísitica e pesos do RW\n",
        "\n",
        "        y_train_pred = clf.predict(x_train_rw) # predição das mesmas entradas utilizadas para treino\n",
        "\n",
        "        pos_ind = np.where(clf.classes_ == df_train.favorable_label)[0][0] # pega índice da classe favoravel\n",
        "\n",
        "\n",
        "        # Validação\n",
        "        # df_valid_pred = df_valid.copy(deepcopy=True) # cria cópia do conjunto de validação\n",
        "        # df_valid_pred.scores = clf.predict_proba(x_valid_rw)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "\n",
        "\n",
        "        # Classification threshold \"converte\" a probabilidade em classes binárias\n",
        "\n",
        "        # calcular acurácia balanceada e outras métricas para todos os limiares\n",
        "        # num_thresh = 100\n",
        "        # ba_arr = np.zeros(num_thresh)\n",
        "        # class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
        "        # metricas = []\n",
        "\n",
        "        # for idx, class_thresh in enumerate(class_thresh_arr):\n",
        "            \n",
        "        # fav_inds = df_valid_pred.scores > bct[i] # Se a probabilidade de classificar a instancia de validação como favorita for maior do que o limiar de classe,\n",
        "        # df_valid_pred.labels[fav_inds] = df_valid_pred.favorable_label # classifica como favorável\n",
        "        # df_valid_pred.labels[~fav_inds] = df_valid_pred.unfavorable_label # se não, classifica como desfavorável\n",
        "        \n",
        "        # metricas.append(ClassificationMetric(df_valid,       # dataset de validação original\n",
        "        #                                     df_valid_pred,  # labels gerados pela probabilidade do classificador\n",
        "        #                                     unprivileged_groups=unprivileged_groups,\n",
        "        #                                     privileged_groups=privileged_groups))\n",
        "        \n",
        "        # ba_arr[idx] = 0.5*(metricas[-1].true_positive_rate()\\\n",
        "                        # +metricas[-1].true_negative_rate()) # calcula a média entre as taxas de verdadeiros positivos e negativos armazenando para cada limiar de classe\n",
        "\n",
        "        # best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
        "        # best_class_thresh = class_thresh_arr[best_ind]\n",
        "\n",
        "\n",
        "        # Teste\n",
        "        df_rw_test_pred = df_test.copy(deepcopy=True) # cria cópia do conjunto de teste\n",
        "        df_rw_test_pred.scores = clf.predict_proba(x_test_rw)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "                    \n",
        "        fav_inds = df_rw_test_pred.scores > bct[i]\n",
        "        df_rw_test_pred.labels[fav_inds] = df_rw_test_pred.favorable_label\n",
        "        df_rw_test_pred.labels[~fav_inds] = df_rw_test_pred.unfavorable_label\n",
        "        \n",
        "        metric_test_aft = compute_metrics(df_test, df_rw_test_pred, \n",
        "                                          unprivileged_groups, privileged_groups,\n",
        "                                          best_class=bct[i],\n",
        "                                          disp = False)\n",
        "        metricas_seeds_rw.append(metric_test_aft)\n",
        "\n",
        "    metrica_df_rw = avg_metrics_seeds(metricas_seeds_rw, name)\n",
        "\n",
        "    return metrica_df_rw\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEV6rVWiw4z9"
      },
      "source": [
        "def treino_DIR(df, clf, scaler, bct, name):\n",
        "    di = DisparateImpactRemover()\n",
        "\n",
        "    metric_df_di = []\n",
        "    for i, s in enumerate(seeds):\n",
        "        np.random.seed(s)\n",
        "        # Separação entre conjunto de teste, treino e validação\n",
        "        df_train, df_test = df.split([0.7], shuffle=True)\n",
        "        # df_valid, df_test = df_vt.split([0.5], shuffle=True)\n",
        "\n",
        "        df_train.features = scaler.fit_transform(df_train.features)\n",
        "        # df_valid.features = scaler.fit_transform(df_valid.features)\n",
        "        df_test.features = scaler.fit_transform(df_test.features)\n",
        "\n",
        "        index = [df_train.feature_names.index(u) for u in list_unprivileged_groups]\n",
        "\n",
        "        df_train_dir = di.fit_transform(df_train)\n",
        "        x_train_dir = np.delete(df_train_dir.features, index, axis=1)\n",
        "        y_train_dir = df_train_dir.labels.ravel()\n",
        "        \n",
        "        # df_valid_dir = di.fit_transform(df_valid)\n",
        "        # x_valid_dir = np.delete(df_valid_dir.features, index, axis=1)\n",
        "\n",
        "        df_test_dir = di.fit_transform(df_test)\n",
        "        x_test_dir = np.delete(df_test_dir.features, index, axis=1)\n",
        "        \n",
        "        clf.fit(x_train_dir, y_train_dir)\n",
        "\n",
        "        pos_ind = np.where(clf.classes_ == df_test.favorable_label)[0][0] # pega índice da classe favoravel\n",
        "\n",
        "        # # Validação\n",
        "        # df_valid_pred = df_valid_dir.copy(deepcopy=True) # cria cópia do conjunto de validação\n",
        "        # x_valid = scaler.transform(df_valid_pred.features) # escalarização do conjunto\n",
        "        # y_valid = df_valid_pred.labels # armazena labels na variável y_valid\n",
        "        # df_valid_pred.scores = clf.predict_proba(x_valid_dir)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "\n",
        "\n",
        "        # Classification threshold \"converte\" a probabilidade em classes binárias\n",
        "\n",
        "        # # calcular acurácia balanceada e outras métricas para todos os limiares\n",
        "        # num_thresh = 100\n",
        "        # ba_arr = np.zeros(num_thresh)\n",
        "        # class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
        "        # metricas = []\n",
        "\n",
        "        # for idx, class_thresh in enumerate(class_thresh_arr):\n",
        "            \n",
        "        #     fav_inds = df_valid_pred.scores > class_thresh # Se a probabilidade de classificar a instancia de validação como favorita for maior do que o limiar de classe,\n",
        "        #     df_valid_pred.labels[fav_inds] = df_valid_pred.favorable_label # classifica como favorável\n",
        "        #     df_valid_pred.labels[~fav_inds] = df_valid_pred.unfavorable_label # se não, classifica como desfavorável\n",
        "            \n",
        "        #     metricas.append(ClassificationMetric(df_valid,       # dataset de validação original\n",
        "        #                                         df_valid_pred,  # labels gerados pela probabilidade do classificador\n",
        "        #                                         unprivileged_groups=unprivileged_groups,\n",
        "        #                                         privileged_groups=privileged_groups))\n",
        "            \n",
        "        #     ba_arr[idx] = 0.5*(metricas[-1].true_positive_rate()\\\n",
        "        #                     +metricas[-1].true_negative_rate()) # calcula a média entre as taxas de verdadeiros positivos e negativos armazenando para cada limiar de classe\n",
        "\n",
        "        # best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
        "        # best_class_thresh = class_thresh_arr[best_ind]\n",
        "\n",
        "\n",
        "        # Treino\n",
        "\n",
        "        df_test_pred = df_test.copy(deepcopy=True)\n",
        "        df_test_pred.scores = clf.predict_proba(x_test_dir)[:,pos_ind].reshape(-1,1) # guarda a probabilidade de a instância ser classificada como rotulo favorável (>50k)\n",
        "\n",
        "        fav_inds = df_test_pred.scores > bct[i]\n",
        "        df_test_pred.labels[fav_inds] = df_test_pred.favorable_label\n",
        "        df_test_pred.labels[~fav_inds] = df_test_pred.unfavorable_label\n",
        "\n",
        "        ##\n",
        "\n",
        "        metric_test_aft = compute_metrics(df_test, df_test_pred, \n",
        "                                            unprivileged_groups, privileged_groups,\n",
        "                                            best_class=bct[i],\n",
        "                                            disp = False)\n",
        "                \n",
        "        # Métrica (conjunto de treino): Statistical Parity Difference\n",
        "        metric_df_di.append(metric_test_aft)\n",
        "    \n",
        "    metrica_df_dir = avg_metrics_seeds(metric_df_di, name)\n",
        "\n",
        "    return metrica_df_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFXWYd31al6y"
      },
      "source": [
        "## Técnicas de pré processamento para mitigação de viés"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FnQeC0zkKRj"
      },
      "source": [
        "#### **Diferença de Paridade Estatística (*Statistical Parity Difference - SPD*)**\n",
        "\n",
        "A distribuição da probabilidade de predição positiva (ou negativa) deve ser idêntica nas subpopulações privilegiadas e desprivilegiadas:\n",
        "\n",
        "$$ SPD = 𝑃𝑟(𝑌=1|𝐷=unprivileged)−𝑃𝑟(𝑌=1|𝐷=privileged) $$\n",
        "\n",
        "SPD é \"mais justo\" quanto mais próximo de 0!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGVBnda_BLH_",
        "outputId": "dc440909-1bd4-45fc-bbd7-75c70923f1e5"
      },
      "source": [
        "metric_df_compas_spd = []\n",
        "\n",
        "for s in seeds:\n",
        "    np.random.seed(s)\n",
        "    # Separação entre conjunto de teste, treino e validação\n",
        "    df_compas_train, df_compas_vt = df_compas.split([0.7], shuffle=True)\n",
        "    #df_compas_valid, df_compas_test = df_compas_vt.split([0.5], shuffle=True)\n",
        "\n",
        "    # Métrica (conjunto de treino): Statistical Parity Difference\n",
        "    metric_df_compas_spd.append(BinaryLabelDatasetMetric(df_compas_train, \n",
        "                                                         unprivileged_groups=unprivileged_groups,\n",
        "                                                         privileged_groups=privileged_groups)\n",
        "                                                        .mean_difference())\n",
        "\n",
        "\n",
        "display(Markdown(\"#### SPD - Adult Dataset Original (grupo discriminado: gênero)\"))\n",
        "print(\"Média da diferença nos resultados médios entre grupos não privilegiados e privilegiados = %f\" % np.mean(metric_df_compas_spd))\n",
        "print(\"Desvio padrão da diferença nos resultados médios entre grupos não privilegiados e privilegiados = %f\" % np.std(metric_df_compas_spd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### SPD - Adult Dataset Original (grupo discriminado: gênero)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Média da diferença nos resultados médios entre grupos não privilegiados e privilegiados = -0.132521\n",
            "Desvio padrão da diferença nos resultados médios entre grupos não privilegiados e privilegiados = 0.009955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9namxuttAarT",
        "outputId": "49d52890-ad04-43f2-f633-aa2a1388d21c"
      },
      "source": [
        "metric_df_compas_spd_rw = []\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "for s in seeds:\n",
        "  np.random.seed(s)\n",
        "  # Separação entre conjunto de teste, treino e validação\n",
        "  df_compas_train, df_compas_vt = df_compas.split([0.7], shuffle=True)\n",
        "  #df_compas_valid, df_compas_test = df_compas_vt.split([0.5], shuffle=True)\n",
        "  \n",
        "  df_compas_rw_train = RW.fit_transform(df_compas_train) # aplicado o reweight no conjunto de treino\n",
        "\n",
        "  metric_df_compas_spd_rw.append(BinaryLabelDatasetMetric(df_compas_rw_train, \n",
        "                                                          unprivileged_groups=unprivileged_groups,\n",
        "                                                          privileged_groups=privileged_groups)\n",
        "                                                          .mean_difference())\n",
        "  \n",
        "display(Markdown(\"#### SPD - Adult Dataset Rebalanceado (grupo discriminado: gênero)\"))\n",
        "print(\"Média da diferença nos resultados médios entre grupos não privilegiados e privilegiados = %f\" % np.mean(metric_df_compas_spd_rw))\n",
        "print(\"Desvio padrão da diferença nos resultados médios entre grupos não privilegiados e privilegiados = %f\" % np.std(metric_df_compas_spd_rw))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### SPD - Adult Dataset Rebalanceado (grupo discriminado: gênero)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Média da diferença nos resultados médios entre grupos não privilegiados e privilegiados = -0.000000\n",
            "Desvio padrão da diferença nos resultados médios entre grupos não privilegiados e privilegiados = 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkC7k2iNGA3d",
        "outputId": "f61b5a08-bbef-4d6a-f8df-340ac05a4bb5"
      },
      "source": [
        "print(df_compas.features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 1. 1. 0.]\n",
            " [0. 1. 1. ... 1. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [1. 0. 1. ... 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Inz2hmkd_9P"
      },
      "source": [
        "#### Regressão Logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wlNweTAUyvR"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RCwnjaChUqa",
        "outputId": "d9ae9714-500b-4d66-b2c1-a3d0cc91fd5c"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "\n",
        "metrica_RL, metrica_RL_sem_valid, threshold_RL = treino_valid_teste(df_compas, lmod, scale_orig, f'{name_unprivileged_groups} - RL - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.658080808080808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6553706087780572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2827499964755467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6054199819102855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6815408630052029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.701702542782617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2506944163057565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20649989260261403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6537037037037038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6534293523849336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.31903794814955594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.5546664236065479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6688872299429872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6651266869427009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2874515317462148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2460234677417236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSLpbhXm-nDR"
      },
      "source": [
        "##### Dataset Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "FL-D2ChL-S5D",
        "outputId": "bf7f45e3-8b4f-4af5-954b-1faf338cb950"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "\n",
        "metrica_RL_RW = treino_RW(df_compas, lmod, scale_transf, threshold_RL, f'{name_unprivileged_groups} - RL - Reweighing')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6523569023569024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6489602999030775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.06519121519707727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.9015924141533596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6782072867076674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7003642259204331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.0300988351475726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.004422181275825802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Q8g6a3dAT8"
      },
      "source": [
        "##### Dataset Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Pp0kr4Sy0F",
        "outputId": "52ed1eb4-eae2-4472-9fcc-65a56bc0d6a5"
      },
      "source": [
        "scaler_orig = MinMaxScaler() #MinMaxScaler()StandardScaler()\n",
        "lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "\n",
        "metrica_RL_DIR = treino_DIR(df_compas, lmod, scaler_orig, threshold_RL, f'{name_unprivileged_groups} - RL - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6628998316498317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6595690172060938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22357566716522095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6722081550130857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6872865011574203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7102523280549249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1887916428638975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.15133153643593023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajR4htdTvbvw"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydt5nlWmkeK6"
      },
      "source": [
        "comparar_metricas([[metrica_RL, 'Original'],\n",
        "                   [metrica_RL_RW, 'Reweighing'],\n",
        "                   [metrica_RL_DIR, 'Disparate Impact Remover']],\n",
        "                  'Regressão Logística', f'{name_unprivileged_groups} - RL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNnGS2PWEQjt"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he9zF_uXEQkQ"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "qvzF-9ypEQkR",
        "outputId": "e5be4a23-7b10-494b-b19e-df90de530270"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "metrica_RF, metrica_RF_sem_valid, threshold_RF = treino_valid_teste(df_compas, RF, scale_orig, f'{name_unprivileged_groups} - RF - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6529882154882156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6511032201931993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2558075538536051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6339740384022926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6759206787435424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6912749193783558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22297331866142947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.19020515369404611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6583333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6543180807115967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.24241877002475695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6691971782324744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6920886651488373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7297766427560162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20957667021410084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.16815003092616806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJN5kcE0Eaaf"
      },
      "source": [
        "##### Dataset Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "LkRzGwN0EQkV",
        "outputId": "bea6f7a4-b2eb-4a94-fe79-a7584f96952d"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "metrica_RF_RW = treino_RW(df_compas, RF, scale_transf, threshold_RF, f'{name_unprivileged_groups} - RF - Reweighing')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6487163299663299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6467352168277982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.05308182070774885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.9238813082845713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6692083435879564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.678291670612707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.017583399420642434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.00934437559397115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPw1q4LBwwyu"
      },
      "source": [
        "##### Dataset Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8HOnmAtwwzQ",
        "outputId": "44e3ce48-375b-4155-f079-c97f79b8f773"
      },
      "source": [
        "scaler_orig = MinMaxScaler()\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "metrica_RF_DIR = treino_DIR(df_compas, RF, scaler_orig, threshold_RF, f'{name_unprivileged_groups} - RF - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6585858585858586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6552989863682197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22363838989262624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6774116477878811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6861890736726542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7139654647737014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.18959606971423598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.15478482904953608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EG4X_Jbvk_L"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nDBN0sOEQkW"
      },
      "source": [
        "comparar_metricas([[metrica_RF, 'Original'],\n",
        "                   [metrica_RF_RW, 'Reweighing'],\n",
        "                   [metrica_RF_DIR, 'Disparate Impact Remover']],\n",
        "                  'Random Forest', f'{name_unprivileged_groups} - RF')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTLxm1kYHPXB"
      },
      "source": [
        "#### Nayve Bayes Bernoulli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUvgRwYOHPXK"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1hCIEBNHPXN",
        "outputId": "b3f0717c-8a43-4f3d-96d5-8b272be8ec9e"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "NB = BernoulliNB()\n",
        "\n",
        "metrica_NB, metrica_NB_sem_valid, threshold_NB = treino_valid_teste(df_compas, NB, scale_orig, f'{name_unprivileged_groups} - NB - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.648358585858586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6472551108807563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.3665210227404846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.5132367836430676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6672131228600376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6722751659121928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.3363936272499825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.30508722051241943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6491161616161617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6469103689912478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.3599327393780062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.5341977306312052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6747108392281034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6915390770323115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.32949861687351795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.3011382245519127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5GE8Z20HPXP"
      },
      "source": [
        "##### Dataset Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "_-IUx4UkHPXQ",
        "outputId": "876666e5-47ed-4571-845d-c7513fafdec9"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "NB = BernoulliNB()\n",
        "\n",
        "metrica_NB_RW = treino_RW(df_compas, NB, scale_transf, threshold_NB, f'{name_unprivileged_groups} - NB - Reweighing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6466750841750842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6450218271115841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20688472157952992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6872492912203885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6686211264178593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6781612935475569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.17405540454274046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.15064220271343634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxpESWjrxGjj"
      },
      "source": [
        "##### Dataset Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z07G4UGxGkA",
        "outputId": "dd7b3d96-48f4-4752-867f-27b0aab32f92"
      },
      "source": [
        "scaler_orig = MinMaxScaler()\n",
        "NB = BernoulliNB()\n",
        "\n",
        "metrica_NB_DIR = treino_DIR(df_compas, NB, scaler_orig, threshold_NB, f'{name_unprivileged_groups} - NB - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6466540404040404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6448900763039813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20724991007881782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6885069828438876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6692070152605182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.679540263566768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.174443072763433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.15089438655582846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3ziyceUwXOC"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w4HERCoHPXR",
        "outputId": "7cfe74c4-6ae6-42ec-8a29-4bfad7670c16"
      },
      "source": [
        "comparar_metricas([[metrica_NB, 'Original'],\n",
        "                   [metrica_NB_RW, 'Reweighing'],\n",
        "                   [metrica_NB_DIR, 'Disparate Impact Remover']],\n",
        "                  'Naive Bayes', f'{name_unprivileged_groups} - NB')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ni-iVfpRvYQ"
      },
      "source": [
        "#### Árvore de decisão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WDmJM2JRvYb"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "dKOcD_SFRvYc",
        "outputId": "ccb1fece-470f-4ded-b601-431b99fd2261"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "metrica_DT, metrica_DT_sem_valid, threshold_DT = treino_valid_teste(df_compas, DT, scale_orig, f'{name_unprivileged_groups} - AD - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6547979797979799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.652394985535148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.25758385301653675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6349998809384835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6788594937929437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6958833212686134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.225009409906259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1884700356358415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.658375420875421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6540654812779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.23767776745877886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6766758404732848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6935332008758834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.734614180024882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20505091037234208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1618461735182499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaIeR8B2RvYd"
      },
      "source": [
        "##### Dataset  Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "Gck41CHWRvYe",
        "outputId": "fc4087c6-c54c-4836-dfb0-d5d60683f159"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "metrica_DT_RW = treino_RW(df_compas, DT, scale_transf, threshold_DT, f'{name_unprivileged_groups} - AD - Reweighing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6487584175084176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6464263675622121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.03569175801931411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.9562735270494481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6690236825856507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6777154948723765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.00016180641556738095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.02688271691123038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYuylxlTxP1S"
      },
      "source": [
        "##### Dataset Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "XXU1hXhgxP1T",
        "outputId": "120be160-1388-4719-eba7-a09d74895003"
      },
      "source": [
        "scaler_orig = MinMaxScaler()\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "metrica_DT_DIR = treino_DIR(df_compas, DT, scaler_orig, threshold_DT, f'{name_unprivileged_groups} - AD - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6596801346801348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6559198102142397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22255570817577683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6821948845284689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6890769141504816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7202201687049757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1885699409103335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.15247641771444473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Clloh12wgYc"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p7dprjpRvYe",
        "outputId": "2d9d0f8f-aa78-4316-d321-673b3be1631c"
      },
      "source": [
        "comparar_metricas([[metrica_DT, 'Original'],\n",
        "                   [metrica_DT_RW, 'Reweighing'],\n",
        "                   [metrica_DT_DIR,'Disparate Impact Remover']],\n",
        "                  'Árvore de decisão', f'{name_unprivileged_groups} - AD')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw122ZXFzuNC"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-H9uU2KzuNG"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a0liKydzuNG",
        "outputId": "962f1466-a0f0-4afc-c16b-f7d2f61f85e8"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "LinearSVM = LinearSVC(dual=False)\n",
        "SVM = CalibratedClassifierCV(LinearSVM)\n",
        "\n",
        "metrica_SVM, metrica_SVM_sem_valid, threshold_SVM = treino_valid_teste(df_compas, SVM, scale_orig, f'{name_unprivileged_groups} - SVM - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.658375420875421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6552603544236655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2772713623559687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6146209889049941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6836334489677647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7071245663809614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2452023584675592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20094474367000764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6602272727272729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6565243385052888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.25659097720508334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6505311173131597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6924819317539429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7270562077581518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2244331434298043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.17342741654624877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M-tk_P3wVL5"
      },
      "source": [
        "##### Dataset Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVy9xvucwVMq",
        "outputId": "adad94f3-a13f-45f2-88fd-de10782c5660"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "LinearSVM = LinearSVC(dual=False)\n",
        "SVM = CalibratedClassifierCV(LinearSVM)\n",
        "\n",
        "metrica_SVM_RW = treino_RW(df_compas, SVM, scale_transf, threshold_SVM, f'{name_unprivileged_groups} - SVM - Reweighing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6533880471380471\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6495531814724411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.08141790753582973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.8772661278646603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6818717472519531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7100823390535658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.04641090232647139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.012396768898109645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlQVzCRzzuNI"
      },
      "source": [
        "##### Dataset transformado pelo Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "fbj6RN_OzuNI",
        "outputId": "d38a7035-0846-42cd-dfbe-40732d3151fd"
      },
      "source": [
        "scaler_orig = MinMaxScaler()\n",
        "LinearSVM = LinearSVC(dual=False)\n",
        "SVM = CalibratedClassifierCV(LinearSVM)\n",
        "\n",
        "metrica_SVM_DIR = treino_DIR(df_compas, SVM, scaler_orig, threshold_SVM, f'{name_unprivileged_groups} - SVM - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6626052188552188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6586983622012208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22337003011687406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.679986728808223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6913082070021905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7220681579321729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.18904199898022503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.14945162341490204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMhXJ3LzuNI"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVaV7CagzuNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59591e6-6cb2-4688-cfab-a4b896474fd8"
      },
      "source": [
        "comparar_metricas([[metrica_SVM, 'Original'],\n",
        "                   [metrica_SVM_RW, 'Reweighing'],\n",
        "                   [metrica_SVM_DIR, 'Disparate Impact Remover']],\n",
        "                  'SVM', f'{name_unprivileged_groups} - SVM') #[metrica_SVM_DIR,'Disparate Impact Remover']], 'SVM')\n",
        "                  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQnH2uLRH4FS"
      },
      "source": [
        "#### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRmVV0n6H4Fd"
      },
      "source": [
        "##### Dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tF6tW7qfH4Fe",
        "outputId": "12998f0a-3d60-4955-d650-f9a94a632450"
      },
      "source": [
        "scale_orig = StandardScaler()\n",
        "MLP_cl = MLPClassifier()\n",
        "MLP = CalibratedClassifierCV(MLP_cl)\n",
        "\n",
        "metrica_MLP, metrica_MLP_sem_valid, threshold_MLP = treino_valid_teste(df_compas, MLP, scale_orig, f'{name_unprivileged_groups} - MLP - Original')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6560185185185187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6539482743993258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.27115777574066746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.615191089673462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6774622817448341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.690021603416664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.23835879566956328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20220023750784252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6602693602693605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6561248626805886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.25349062048985876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6577040148511583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6942516300531645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7331347251937995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.22039473501101053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1782301274257806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRQ3UjUfuHEx"
      },
      "source": [
        "##### Dataset Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tWHkUR9tuHFW",
        "outputId": "6e31fc92-08e7-47d4-bde6-8bb955e26baa"
      },
      "source": [
        "scale_transf = StandardScaler()\n",
        "MLP_cl = MLPClassifier()\n",
        "MLP = CalibratedClassifierCV(MLP_cl)\n",
        "\n",
        "metrica_MLP_RW = treino_RW(df_compas, MLP, scale_transf, threshold_MLP, f'{name_unprivileged_groups} - MLP - Reweighing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/calibration.py:175: UserWarning: MLPClassifier does not support sample_weight. Samples weights are only used for the calibration itself.\n",
            "  \" itself.\" % estimator_name)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6603745791245792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6584354377729251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2672428580493996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.617816519244177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6812722895092146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6945816977445918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.23227768533151208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.20403003556000032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_1v2-wH4Fi"
      },
      "source": [
        "##### Dataset transformado pelo Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5qRUzQH4Fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "1ba8d25a-d462-40d1-9f34-5843d2b9165b"
      },
      "source": [
        "scaler_orig = MinMaxScaler()\n",
        "MLP_cl = MLPClassifier()\n",
        "MLP = CalibratedClassifierCV(MLP_cl)\n",
        "\n",
        "metrica_MLP_DIR = treino_DIR(df_compas, MLP, scaler_orig, threshold_MLP, f'{name_unprivileged_groups} - MLP - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6598063973063972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Acurácia Balanceada (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6570223694334786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença Paridade estatística (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.2262701527586087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Impacto diferenciado (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6708668563119734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### F1 Score (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.6853115428455615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Recall (Objetivo: 1)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7086008162223808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Diferença de probabilidades (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1920171357680696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Equal opportunities difference (Objetivo: 0)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.1554348361393777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHynRjRvH4Fl"
      },
      "source": [
        "##### Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPrIr0nH4Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3608207-9696-40a3-a2fb-2225a3cdd532"
      },
      "source": [
        "comparar_metricas([[metrica_MLP, 'Original'],\n",
        "                   [metrica_MLP_RW, 'Reweighing'],\n",
        "                   [metrica_MLP_DIR,'Disparate Impact Remover']],\n",
        "                  'MLP', f'{name_unprivileged_groups} - MLP')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu8TSPJ8OGK8"
      },
      "source": [
        "### Comparativo entre todos os algoritmos rodando no dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86NkMvXKOMd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7b8dac-3a83-4661-b3e6-cf745f5c37fd"
      },
      "source": [
        "comparar_metricas([[metrica_RL, 'Regressão Logística'],\n",
        "                   [metrica_RF, 'Random Forest'],\n",
        "                   [metrica_NB, 'Nayve Bayes'],\n",
        "                   [metrica_DT, 'Árvore de Decisão'],\n",
        "                   [metrica_SVM, 'SVM'],\n",
        "                   [metrica_MLP, 'MLP']],\n",
        "                   'Algoritmos', f'{name_unprivileged_groups} - Sem debiasing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-GJZxqRR0J"
      },
      "source": [
        "### Comparativo entre todos os algoritmos rodando no dataset original - Sem validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QNSeT58RR0J",
        "outputId": "33aa0ce5-f1e0-48d3-ba7c-06dcb96f9a8c"
      },
      "source": [
        "comparar_metricas([[metrica_RL_sem_valid, 'Regressão Logística'],\n",
        "                   [metrica_RF_sem_valid, 'Random Forest'],\n",
        "                   [metrica_NB_sem_valid, 'Nayve Bayes'],\n",
        "                   [metrica_DT_sem_valid, 'Árvore de Decisão'],\n",
        "                   [metrica_SVM_sem_valid, 'SVM'],\n",
        "                   [metrica_MLP_sem_valid, 'MLP']],\n",
        "                   'Algoritmos', f'{name_unprivileged_groups} - Sem debiasing - Sem Validação')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K98gKAm2tR2V"
      },
      "source": [
        "### Comparativo entre todos os algoritmos utilizando Reweighing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGYplSLStZw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec05e7d-26a7-4d36-a4f1-990d66a891fa"
      },
      "source": [
        "comparar_metricas([[metrica_RL_RW, 'Regressão Logística'],\n",
        "                   [metrica_RF_RW, 'Random Forest'],\n",
        "                   [metrica_NB_RW, 'Nayve Bayes'],\n",
        "                   [metrica_DT_RW, 'Árvore de Decisão'],\n",
        "                   [metrica_SVM_RW, 'SVM'],\n",
        "                   [metrica_MLP_RW, 'MLP']],\n",
        "                   'Algoritmos', f'{name_unprivileged_groups} - Reweighing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf5AYMZqtwGm"
      },
      "source": [
        "### Comparativo entre todos os algoritmos utilizando Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfhr-gOSt1rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f127ec-4c18-404c-cdb3-13f3e94b2cd1"
      },
      "source": [
        "comparar_metricas([[metrica_RL_DIR, 'Regressão Logística'],\n",
        "                   [metrica_RF_DIR, 'Random Forest'],\n",
        "                   [metrica_NB_DIR, 'Nayve Bayes'],\n",
        "                   [metrica_DT_DIR, 'Árvore de Decisão'],\n",
        "                   [metrica_SVM_DIR, 'SVM'],\n",
        "                   [metrica_MLP_DIR, 'MLP']],\n",
        "                   'Algoritmos', f'{name_unprivileged_groups} - Disparate Impact Remover')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}